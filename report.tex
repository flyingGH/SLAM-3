\documentclass[english]{article}

%I think these are useful for pdftex compatibility. I bastardised an old latex report.
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}

\usepackage[a4paper]{geometry}

%for manual adjustment of borders if necessary
%\geometry{verbose,tmargin=1.5cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{algorithm,algorithmic}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage{babel}


%A quick note about style - for consistency, I propose using postscript for images and producing any mathematical graphs and figures in Mathematica. Also, make sure you add any figures, mathematica files or anything to git.
%I think you have more experience with LaTeX layout than me. I'm not so sure stylistically about things such as numbering of sections.

\begin{document}

%Working title? This will be incorporated into our final report, but right now it's just a mini-report. Since Kalman and Particle Filters are essentially just different implementations of Bayes' Filter, I think "Bayesian Filtering" is appropriate".
\title{Bayesian Filtering}

\author{Duncan Burke and Sebastian Pauka}
\maketitle

%Do we need an abstract at this point? It isn't original research, after all.

\section*{Introduction}


A fundamental requirement for any robotics sytem is the ability to percieve and interact with its physical environment. Any reasonable interaction of a robot with its environment requires an internal model of its surroundings for purposes such as robot localisation, map generation, planning and to enable any other sensor processing to be placed in its spatial context. The creation of this model requires \emph{data fusion} - some algorithm  enabling a multitude of data sources and measurements to produce a single probabilistic estimate. It must also be considered that the data incorporated into such a model is inherently noisy and may further suffer from systematic error, and that in realistic applications algorithimic approximations are commonly necessary for computational tractability\cite{probrob}.

This model is call the \emph{state} and is given in discrete steps by the variable $x_t$, indexed by time $t$. In addition, at time $t$, control data $u_t$ and measurements $z_t$ are recieved. It is assumed that commands and measurements occur in discrete steps with the measurement $z_t$ taken after the preceding control command given by $u_t$ has been completed. The state $x_t$ is not directly observable, however it is generated stochastically from the previous states $x_{0:t-1}$ and movement commands $u_{1:t-1}$\cite{Thrun02d}. If  $x_t$ is conditionally independent of $x_{0:t-2}$ and $u_{1:t-2}$ given $x_{t-1}$, x is said to be \emph{complete} and satisfies the \emph{Markov Condition}\cite{probrob}. Although $x_t$ is hidden, the measurement $z_t$ is generated stochastically from $x_t$ allowing for indirect observation\cite{Thrun02d}.

The belief distrubtion $bel(x_t)$ of the state $x_t$ assigns a probability to every possible state that it is the actual hidden state. The belief is defined conditionally on all past movement commands and sensor measurements\cite{probrob} (Equation \ref{eq:bel}). It is also convenient to define the belief in $x_t$ prior to incorporating the measurement $z_t$ generated from $x_t$ (Equation \ref{eq:prediction}); this is referred to as the \emph{prediction} and is used to generate $bel(x_t)$ by incorporating $z_t$ in a \emph{measurement update}.
\begin {align}
  bel(x_t) & = p(x_t | z_{1:t},u_{1:t}) \label{eq:bel} \\   
  \overline{bel}(x_t) & = p(x_t | z_{1:t-1}, u_{1:t}) \label{eq:prediction}
\end {align}

%At their core, filters aim to combine observable processes, in our case $u_t$ and $z_t$, into an unobservable signal $x_t$. We can state this mathematically as suggesting that $u_t$ and $z_t$ are observable random variables, and the aim is to synthesise the unobservable random variable $x_t$ via $\left\{u_s,z_s; s \le 1\right\}$. Of course, this process may be computationally very difficult, and the appropriate method to use depends on the nature of the random variables, and the expected nature of the output.

\section*{Bayes Filter}
The Bayes Filter is an algorithm used to calculate the belief $bel(x_t)$. It is assumed that the state is complete. Therefore, the algorithm can be framed recursively as an \emph{update rule} calculating the new belief $bel{x_t}$ given the previous belief $bel(x_{t-1})$ and new inputs $u_t$ and $z_t$. The algorithm consists of two core steps: the \emph{control update} and the \emph{measurement update}. Firstly, for every state $x_t$ in the statespace, an updated prediction $\overline{bel}(x_t)$ is given by the integral of the conditional probability of $x_t$ given $bel(x_{t-1})$ and $u_t$\cite{probrob}. This requires the \emph{movement model} $p(x_t \mid u_t,x_{t-1})$ and may be given by a summation in the case of a discrete statespace. $bel(x_t)$ can now be calculated from the \emph{measurement model} $p(z_t \mid x_t)$ and $\overline{bel}(x_t)$.

\begin{algorithm}
\caption{Bayes Filter}
\label{alg:bayes}
\begin{algorithmic}
	\REQUIRE $bel(x_{t-1}), u_t, z_t$
        \FOR {all $x_t$}
        \STATE $\overline{bel}(x_t)  = \int p(x_t \mid u_t, x_{t-1})bel(x_{t-1}) dx_{t-1}$
        \STATE $bel(x_t)  = \eta p(z_t \mid x_t) \overline{bel}(x_t)$
        \ENDFOR
        \RETURN $bel(x_t)$
\end{algorithmic}
\end{algorithm}



 
%; in practice, this may not be strictly necessary and acceptable results may be achieved with a state which is not complete\cite{probrob}


%We need to cover this is some detail, I think. Fabio seems to be heavily interested in the theoretical side of robotics so we should at the very least justify and explain the algorithm and why it is impractical in its given form.
%\begin{equation}
%\label{eq:mvarnormal}
%p(x) = \det(2 \pi \Sigma)^{-1/2} \exp\left\{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\}
%\end{equation}


\section*{Kalman Filter}

One of the most common implementations of the Bayes filter is the Kalman Filter. The Kalman filter is a technique for filtering and prediction in linear Gaussian systems. Dispite this limitation, the Kalman filter is one of the best studied techniques for implementing the Bayes filters.

The filter relies on a number of assumptions about the inputs, above those made by the Bayes' Filter, namely:
\begin{enumerate}
	\item The state transition probability $p(x_t | u_t,x_{t-1})$ is a linear function with added Gaussian noise. A linear Gaussian is expressed as:
		\begin{equation}
			x_t = A_t x_{t-1} + B_t u_t + \epsilon _t
		\end{equation}
		Since $A_t$ and $B_t$ are constant, the state transition function is linear in its arguments.
	\item The measurement propability must also be linear in its arguments, again with added Gaussian nose. It is expressed as:
		\begin{equation}
			z_t = C_t x_t + \delta _t
		\end{equation}
	\item The initial belief $bel(x_0)$ must be normally distributed. This is denoted:
		\begin{equation}
			bel(x_0) = p(x_0) = \det(2 \pi \Sigma_0)^{-1/2} \exp\left\{-\frac{1}{2}(x_0-\mu_0)^T \Sigma_0^{-1}(x_0-\mu_0)\right\}
		\end{equation}
		such that the mean of the belief is $\mu_0$ and the covariance $\Sigma_0$.
\end{enumerate}
Under these assumptions, we can show that for any $t$, the posterior $bel(x_t)$ is always Gaussian.

The constant terms of the state transition probability, $A_t$ and $B_t$ are matrices which represent some physical relationship between the terms of the state vector ($x_t$) and the control vector $(u_t)$. As such, if the dimension of the state vector is $n$ and the dimension of the conrol vector is $m$ then the matrix $A_t$ is a $n \times n$ matrix, and the matrix $B_t$ is a $n \times m$ matrix, such that the posterior state vector is also a vector of dimension $n$. 

The random variable $\epsilon _t$ is a Gaussian random vector of dimension $n$ that represents the uncertainty introduced by the state transition. It has a mean of zero, and covariance $R_t$.

Therefore, the distribution of $p(x_t | u_t, x_{t-1})$ has a mean of $A_t x_{t-1} + B_T u_t$ and covariance $R_t$, and therefore by substitution into the definition of the multivariate normal distribution (eq ~\ref{eq:mvarnormal}):
\begin{equation}
	p(x_t | u_t, x_{t-1}) = \det(2 \pi R_t)^{-1/2} \exp\left\{-\frac{1}{2}(x_t - A_t x_t-1 - B_t u_t)^T R_t^{-1}(x_t - A_t x_{t-1} - B_t u_t)\right\}
\end{equation}

Similarly, for a measurement vector $z_t$ of size $k$, $C_t$ is a matrix of size $k \times n$, and $\delta_t$ describes mesaurement noise. The measurement noise is a multivariate Gaussian with zero mean and covariance $Q_t$, such that, again my substitution into the multivariate normal distribution is:
\begin{equation}
	p(z_t | x_t) = \det(2 \pi Q_t)^{-1/2} \exp\left\{-\frac{1}{2}(z_t - C_t x_t)^T Q_t^{-1}(z_t - C_t x_t)\right\}
\end{equation}

The algorithm for the Kalman filter is thus:

\begin{algorithm}
\caption{Kahlman Filter Algorithm}
\label{alg:kalman}
\begin{algorithmic}
	\REQUIRE $\mu_{t-1}, \Sigma_{t-1}, u_t, z_t$
	\STATE $\overline{\mu}_t \leftarrow A_t\mu_{t-1} + B_t \mu_t$
	\STATE $\overline{\Sigma}_t \leftarrow A_t \Sigma_{t-1}A_t^T + R_t$
	\STATE
	\STATE $K_t \leftarrow \overline{\Sigma}_t C_t^T\left(C_t \overline{\Sigma}_t C_t^T + Q_t\right)^{-1}$
	\STATE $\mu_t \leftarrow \overline{\mu}_t + K_t\left(z_t - C_t \overline{\mu}_t\right)$
	\STATE $\Sigma_t \leftarrow (I-K_t C_t)\overline{\Sigma}_t$
	\RETURN $\mu_t, \Sigma_t$
\end{algorithmic}
\end{algorithm}

\section*{Particle Filter}

%Overview of a particle filter - state represented as a sampling of possible states
Whereas the Kalman Filter represents the belief as a paramaterised gaussian distribution, the Particle Filter is non-parameterised, representing the belief at time $t$ as a finite set $\chi_t$ of $M$ states $\{x^{[1]}_t, x^{[2]}_t, \cdot , x^{[M]}_t\}$ drawn from the state space according to the distribution of the posterior. By increasing the size of the particle filter, this can be made to approximate any distribution without any regards to constraints such as unimodality.

At the start of the algorithm, the set of particles $\chi_0$ are drawn according to the distribution of the start state $p(x_0)$ (Equation \ref{eq:particle_initial}. At time $t$, a new hypothetical state $x^{[m]}_t$ is generated from each particle $x^{[m]}_{[t-1]}$ for each $m \in [1,M]$ by sampling a state from the distribution of the state transition probability (Equation \ref{eq:particle_prediction}). These new states belong to $\bar{\chi}_t$. Next, a resampling step randomly selects with replacement $M$ particles from $\bar{\chi}_t$ into $\chi_t$ with weights $w^{[m]}_t$ given by the measurement model (Equation \ref{eq:particle_weight}).

\begin {align}
  x^{[m]}_0 \sim  p(x_0) \label{eq:particle_initial}\\
  x^{[m]}_t \sim p(x_t \mid u_t,x^{[m]}_{t-1}) \label{eq:particle_prediction} \\
  w^{[m]}_t = \eta p(z_t \mid \bar{x}^{[m]}_t) \label{eq:particle_weight}
\end {align}

Hence, the complete algorithm is:
\begin{algorithm}
\caption{Particle Filter Algorithm}
\label{alg:particla}
\begin{algorithmic}
	\REQUIRE $\chi_{t-1}, u_t, z_t$
        \STATE $\bar{\chi}_t = \chi_t = \emptyset$
        \FOR {$m = 1 to M$}
        \STATE $sample \bar{x}^{[m]}_t \sim p(x_t \mid u_t,x^{[m]}_{t-1})$
        \STATE $w^{[m]}_t = p(z_t | x^{[m]}_t)$
        \STATE Add $\bar{x}^{[m]}_t$ and $w^{[m]}_t$ to $\bar{\chi}_t$
        \ENDFOR

        \FOR {$m = 1 to M$}
        \STATE Select an index $i \in [1,m]$ with probability $\eta w^{[i]}_t$
        \STATE Add $x^{[i]}_t$ to $\chi_t$
        \ENDFOR

\end{algorithmic}
\end{algorithm}


Following this algorithm, asymptotically $\lim_{M \to \infty} x^{[i]}_t \ sim p(x_t \mid z_{1:t}, u_{1:t})$.

%Weighting

%Resampling

\section*{Discussion}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
