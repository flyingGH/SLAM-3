\documentclass[english]{article}

%I think these are useful for pdftex compatibility. I bastardised an old latex report.
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}

\usepackage[a4paper]{geometry}

%for manual adjustment of borders if necessary
%\geometry{verbose,tmargin=1.5cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{fullpage}
\usepackage{verbatim}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{subfig}
\usepackage{babel}


%A quick note about style - for consistency, I propose using postscript for images and producing any mathematical graphs and figures in Mathematica. Also, make sure you add any figures, mathematica files or anything to git.
%I think you have more experience with LaTeX layout than me. I'm not so sure stylistically about things such as numbering of sections.

\begin{document}

%Working title? This will be incorporated into our final report, but right now it's just a mini-report. Since Kalman and Particle Filters are essentially just different implementations of Bayes' Filter, I think "Bayesian Filtering" is appropriate".
\title{Bayesian Filtering}

\author{Duncan Burke and Sebastian Pauka}
\maketitle

%Do we need an abstract at this point? It isn't original research, after all.

\section*{Introduction}

A fundamental requirement for any robotics sytem is the ability to percieve and interact with its physical environment. Any reasonable interaction of a robot with its environment requires an internal model of its surroundings for purposes such as robot localisation, map generation, planning and to enable any other sensor processing to be placed in a spatial context. The data incorporated into such a model from sources such as odometry and sensors is noisy and may suffer from systemeatic error; this is compounded in realistic applications by the need for algorithmic approximations, producing an imperfect abstraction of the robot's environment described by a probability distribution.

%Brief mention of Bayes' theorem. At what level are we meant to pitch this?
%Structure of the problem - x, u and z. Why this produces a HMM and the markov assumption.

%Maybe find a reference for the markov property and/or conditional independece
%NEEDS a reference to The Book
At time t, the robot's state is represented by the variable $x_t$, the control data by $u_t$ and sensor data by $z_t$. Na\"{i}vely, $x_t$ may be expressed conditionally as a function of all previous states, sensor and odometry data: $P(x_t | x_{0:t-1}, u_{0:t}, z_{0:t})$. The tractability of this may be improved by ensuring or assuming that $x$ is 'complete', that it satisifes the Markov Condition requiring $x_t$ to be conditionally independent of $u_{0:t-1}$ and $z_{0:t-1}$ given complete knowledge of the immediately preceding state $x_{t-1}$. Equivalently, this may be expressed as $x_t$ containing all preceding information contributing to the conditional probabilty of future states.

As a matter of convenience, it is assumed that events occur in the following order:
\begin{enumerate}
  \item A movmement $u_t$ is executed 
  \item An updated prediction $\overline{bel}(x_t)$ of the state $x$ is formed based on the movement instruction
  \item A measurement $z_t$ is taken
  \item The measurement information is used to create a posterior belief $bel(x_t)$ from the prediction $\overline{bel}(x_t)$
\end{enumerate}

At their core, filters aim to combine observable processes, in our case $u_t$ and $z_t$, into an unobservable signal $x_t$. We can state this mathematically as suggesting that $u_t$ and $z_t$ are observable random variables, and the aim is to synthesise the unobservable random variable $x_t$ via $\left\{u_s,z_s; s \le 1\right\}$. Of course, this process may be computationally very difficult, and the appropriate method to use depends on the nature of the random variables, and the expected nature of the output.

\section*{Bayes' Filter}

%We need to cover this is some detail, I think. Fabio seems to be heavily interested in the theoretical side of robotics so we should at the very least justify and explain the algorithm and why it is impractical in its given form.
\begin{equation}
\label{eq:mvarnormal}
p(x) = \det(2 \pi \Sigma)^{-1/2} \exp\left\{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\}
\end{equation}

\section*{Kalman Filter}

One of the most common implementations of the Bayes filter is the Kalman Filter. The Kalman filter is a technique for filtering and prediction in linear Gaussian systems. Dispite this limitation, the Kalman filter is one of the best studied techniques for implementing the Bayes filters.

The filter relies on a number of assumptions about the inputs, above those made by the Bayes' Filter, namely:
\begin{enumerate}
	\item The state transition probability $p(x_t | u_t,x_{t-1})$ is a linear function with added Gaussian noise. A linear Gaussian is expressed as:
		\begin{equation}
			x_t = A_t x_{t-1} + B_t u_t + \epsilon _t
		\end{equation}
		Since $A_t$ and $B_t$ are constant, the state transition function is linear in its arguments.
	\item The measurement propability must also be linear in its arguments, again with added Gaussian nose. It is expressed as:
		\begin{equation}
			z_t = C_t x_t + \delta _t
		\end{equation}
	\item The initial belief $bel(x_0)$ must be normally distributed. This is denoted:
		\begin{equation}
			bel(x_0) = p(x_0) = \det(2 \pi \Sigma_0)^{-1/2} \exp\left\{-\frac{1}{2}(x_0-\mu_0)^T \Sigma_0^{-1}(x_0-\mu_0)\right\}
		\end{equation}
		such that the mean of the belief is $\mu_0$ and the covariance $\Sigma_0$.
\end{enumerate}
Under these assumptions, we can show that for any $t$, the posterior $bel(x_t)$ is always Gaussian.

The constant terms of the state transition probability, $A_t$ and $B_t$ are matrices which represent some physical relationship between the terms of the state vector ($x_t$) and the control vector $(u_t)$. As such, if the dimension of the state vector is $n$ and the dimension of the conrol vector is $m$ then the matrix $A_t$ is a $n \times n$ matrix, and the matrix $B_t$ is a $n \times m$ matrix, such that the posterior state vector is also a vector of dimension $n$. 

The random variable $\epsilon _t$ is a Gaussian random vector of dimension $n$ that represents the uncertainty introduced by the state transition. It has a mean of zero, and covariance $R_t$.

Therefore, the distribution of $p(x_t | u_t, x_{t-1})$ has a mean of $A_t x_{t-1} + B_T u_t$ and covariance $R_t$, and therefore by substitution into the definition of the multivariate normal distribution (eq ~\ref{eq:mvarnormal}):
\begin{equation}
	p(x_t | u_t, x_{t-1}) = \det(2 \pi R_t)^{-1/2} \exp\left\{-\frac{1}{2}(x_t - A_t x_t-1 - B_t u_t)^T R_t^{-1}(x_t - A_t x_{t-1} - B_t u_t)\right\}
\end{equation}

Similarly, for a measurement vector $z_t$ of size $k$, $C_t$ is a matrix of size $k \times n$, and $\delta_t$ describes mesaurement noise. The measurement noise is a multivariate Gaussian with zero mean and covariance $Q_t$, such that, again my substitution into the multivariate normal distribution is:
\begin{equation}
	p(z_t | x_t) = \det(2 \pi Q_t)^{-1/2} \exp\left\{-\frac{1}{2}(z_t - C_t x_t)^T Q_t^{-1}(z_t - C_t x_t)\right\}
\end{equation}

%You wanted to do this, so I'll leave it to you. Remember, put anything about the Bayes' filter in general in the Bayes' filter section and leave a comparison of techniques to Discussion.
%Though I think it is still a good idea to go over the advantages and disadvantages in this section.

\section*{Particle Filter}

%Overview of a particle filter - state represented as a sampling of possible states
A particle filter represents the state $x_t$ not as a paramaterised probability distribution, but as a finite number of representative states
%Algorithm

%Weighting

%Resampling

\section*{Discussion}


\end{document}
